---
title: "Introduction to ARIMA Models Using R"
author: "Laura Rose"
date: "April 19, 2022"
output: 
  xaringan::moon_reader:
    css: [default, rladies-fonts, rladies]
    lib_dir: libs
    nature:
      countIncrementalSlides: false
      ratio: '16:9'
      
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, fig.retina = 3, warning = FALSE, message = FALSE)
package_list <- c("fable", "feasts", "tsibble", "tidyverse")
for (i in package_list){
  if (!require(i, character.only = TRUE)){
    install.packages(i, repos = 'http://cran.us.r-project.org', dependencies = TRUE)
  library(i, character.only = TRUE)
  }
}

```

## Intro to ARIMA Models
- ARIMA models are one of the most common univariate time series forecasting methods.

--

- ARIMA stands for **A**uto**R**egressive **I**ntegrated **M**oving **A**verage.

--

- We will explore each part of the ARIMA model in detail before moving on to its implementation in R.

---

## For Further Reading
- Hyndman, R.J., & Athanasopoulos, G. (2021) *Forecasting: principles and practice*, 3rd edition, OTexts: Melbourne, Australia. OTexts.com/fpp3. Accessed on March 3, 2022.
- Brooks, C. (2008). *Introductory Econometrics for Finance*, 2nd edition, Cambridge University Press: Cambridge, United Kingdom.

---

## Stationarity
- A time series is said to be *strictly stationary* if the distribution of the observations does not change across time.

--

- However, strict stationarity is not always necessary to model a time series. We often reference *weak stationarity* in our analysis.

--

- $E(y_{t}) = \mu$ (constant mean)

--

- $E[(y_{t} - \mu)(y_{t} - \mu)] = \sigma^2 < \infty$ (constant variance)

--

- $E[(y_{t_{k}} - \mu)(y_{t_{k'}} - \mu)] = \gamma_{t_{k}-t_{k'}} \space \forall \space t_{k}, t_{k'}$ (constant autocovariance)

--
  - When $k = k'$, the autocovariance is the variance.
  
--

- The autocovariance is not particularly useful since it is scale-dependent. Therefore, we often normalize the autocovariance by dividing by the variance.

--

- This is called the *autocorrelation*, and this measure has the usefulness of being bounded between $\pm1$

---

## White Noise Process

- A *white noise process* is a special case of a stationary process.

--

$$E(y_{t}) = 0$$

--

$$E[(y_{t} - \mu)(y_{t} - \mu)] = \sigma^2$$

--

$$\gamma_{k-k'} = \left\{\begin{aligned}
&\sigma^2 &&if \space k = k'\\
&0 && otherwise
\end{aligned}
\right.$$

--

- *Note that some definitions of a white noise process indicate a nonzero mean is permissible.*

---

## Random Walk 

- A *random walk* model is often used to denote a nonstationary time series.

--

- $y_{t} = y_{t-1} + \varepsilon_{t}$ or

--

- $y_{t} = c + y_{t-1} + \varepsilon_{t}$ (random walk w/drift)
  - $c$ is the mean of the changes between sequential observations.
  - If $c > 0$, the mean change will tend towards an increase in $y_{t}$, and vice versa for negative values of $c$. 
  
--

- Random walk series often have long period of increases or decreases accompanied with sudden changes in direction.

---

## What to Do If Data is Nonstationary?
- So what happens if we find our data violates any of the (weak) stationarity conditions?

--

- Note that data with trend or seasonality is not stationary, but data with cyclic behavior (i.e., business or other cycles which are not of fixed length) can be stationary.

--

- Differencing the series is a common way to deal with nonstationarity resulting from autocorrelation. 

--

  - The difference of a value with the previous value is called *first-order differencing*: $y'_{t} = y_{t} - y_{t-1}$.
  - Note that for a random walk model, $y'_{t} = \varepsilon_{t}$. 
  - Thus, this series is now stationary given the properties of $\varepsilon_{t}$.
  
--

  - Sometimes this is not enough to make the series stationary, so we take the difference of the differences.

--

$$\begin{align*}
  y''_{t}  &=  y'_{t}  - y'_{t - 1} \\
           &= (y_t - y_{t-1}) - (y_{t-1}-y_{t-2})\\
           &= y_t - 2y_{t-1} +y_{t-2}
\end{align*}$$

---

## What to Do if Data is Nonstationary, cont.

- If we notice a seasonal pattern, we can take a seasonal difference: $y'_{t} = y_{t} - y_{t-m}$, where $m$ is the seasonal period. 

--

- If the data looks like it may require both first-differencing and seasonal differencing, it's better to take the seasonal difference first.

--

- This is especially true in the case of strong seasonality, since sometimes taking a seasonal difference is enough to make the series stationary.

--

- However, taking a first difference will not get rid of seasonal stationarity.

--

- Always use as few differences as possible, since too much differencing can induce patterns in the series that are not actually there.

--

- If the variance appears nonconstant across time, a logarithmic transformation can be used to stabilize.

---

## How to Test for Nonstationarity

- There are both formal and nonformal ways to examine the time series.

--

- We should always start by plotting the time series (line plot is generally best), since often nonstationarity and nonconstant variance are visible to the human eye.

--

- If we suspect nonstationarity, we should plot the autocorrelation function (ACF) and check for observations outside of the 95% confidence interval around 0. We particularly want to examine how quickly these autocorrelations drop to 0.

--

- To formally test for stationarity, we conduct a *unit root test*.

--

- Taking a cue from Dr. Hyndman, we will use the KPSS test, but other tests are available (ADF and PP tests).

--

- A *unit root* implies that the data is essentially a function of its lag(s) where the coefficient equals 1, plus some noise. (There are some variations on this, but we will skip discussion for the sake of simplicity.)
  - In more formal terms, the root of the characteristic equation equals 1.

---

## An Example of Checking for Stationarity

.pull-left[
```{r plot-last, fig.show='hide', warning=FALSE, message=FALSE}
data(BJsales)
date_sequence <- seq(as.Date("1955-01-01"), 
by = "month", length.out = 150)
Sales_tsib <- tsibble(Month = yearmonth(date_sequence), 
Sales = BJsales, index = Month)
autoplot(Sales_tsib)
```
]

.pull-right[
```{r ref.label='plot-last', echo=FALSE}
```
]

---

## Checking the ACF

.pull-left[
```{r plot-last2, fig.show='hide'}
Sales_tsib %>% ACF() %>% autoplot()
```
]

--

.pull-right[
```{r ref.label='plot-last2', echo=FALSE}
```
]

---

## Testing for a Unit Root 

```{r uroot}
uroot_table <- Sales_tsib %>% 
features(Sales, unitroot_kpss) %>% 
bind_cols(Sales_tsib %>%
features(Sales, unitroot_ndiffs)) %>% 
bind_cols(Sales_tsib %>% 
features(Sales, unitroot_nsdiffs))
knitr::kable(uroot_table, format = "html",
digits = 2, align = "c")
```

--

- We clearly have a unit root (we reject the null hypothesis of stationarity), but it doesn't look like we will need to take a seasonal difference.

---

## Backshift Notation

- This notation is a (hopefully!) helpful shorthand to describe lags of variables for easier manipulation. Think of $B$ as "backshift."

--

- $By_{t} = y_{t-1}$

--

- By extension, $B(By_{t}) = B^2y_{t} = y_{t-2}$, etc.

--

- Same quarter last year would be $B^4y_{t} = y_{t-4}$

--

- We can also use the backshift operator to describe differencing.
$$y' = y_{t} - y_{t-1} = y_{t} - By_{t} = (1-B)y_{t}$$
- We can treat the expressions in $B$ like polynomials and solve for roots (thus the unit root concept).
  
--

$$\begin{align*}
y'' &= (y_{t} - y_{t-1}) - (y_{t-1} - y_{t-2})\\ 
    &= y_{t} - By_{t} - By_{t} + B^2y_{t}\\ 
    &= y_{t} - 2By_{t} + B^2y_{t}\\ 
    &= (B-1)^2y_{t}\\ 
    &= (1-B)^2y_{t}
\end{align*}$$
  
---

## Autoregressive (AR) Models

- With an AR model, we forecast a time series using a linear combination of prior values of the series. In other words, $y_{t}$ is regressed on itself.

--

- An autoregressive model of order $p$ (an $AR(p)$) model is written as follows, where $\varepsilon_{t}$ is a white noise process:

--

$$y_{t} = c + \phi_{1}y_{t-1} + \phi_{2}y_{t-2} + ... + \phi_{p}y_{t-p} + \varepsilon_{t}$$
--

- $AR(1)$ models as they relate to previous models we've discussed:
  - when $\phi_{1} = 0$ and $c = 0$, $y_{t}$ is white noise
  - when $\phi_{1} = 1$ and $c = 0$, $y_{t}$ is a random walk process
  - when $\phi_{1} = 1$ and $c \neq 0$, $y_{t}$ is a random walk with drift process
  - when $\phi_{1} < 0$, $y_{t}$ tends to fluctuate around the mean

--

- $AR(p)$ models, stationary data, and constraints:
  - $AR(1)$ model: $-1 < \phi_{1} < 1$
  - $AR(2)$ model: $-1 < \phi_{2} < 1$, $\phi_{1} + \phi_{2} < 1$, $\phi_{1} - \phi_{2} < 1$

--

- These restrictions, as well as restrictions for higher orders of $p$, are handled automatically in the `fable` package.

---

## An Example of AR(1) Process

.pull-left[
```{r ar-process, fig.show='hide'}
set.seed(5)
arima.sim(list(order = c(1, 0, 0),
ar = 0.7), n = 300) %>% as_tsibble() %>%
autoplot(value)
```
]

--

.pull-right[
```{r ref.label='ar-process', echo=FALSE}
```
]

---

## Another Example of AR(1)<sup>*</sup> Process

.pull-left[
```{r ar-process-1, fig.show='hide'}
set.seed(5)
arima.sim(list(order = c(1, 1, 0),
ar = 0.7), n = 300) %>%
as_tsibble() %>%
autoplot(value)
```
]

--

.pull-right[
```{r ref.label='ar-process-1', echo=FALSE}
```
]

.footnote[[*]<small>This is technically not an AR(1) model,<br> but rather an ARIMA(1,1,0).<br> We can see the difference in a stationary versus nonstationary series.</small>]

---

## An Example of an AR(2) Process

.pull-left[
```{r ar-process-2, fig.show='hide'}
set.seed(5)
arima.sim(list(order = c(2, 0, 0),
ar = c(1.3, -0.6)), n = 300) %>%
as_tsibble() %>%
autoplot(value)
```
]

--

.pull-right[
```{r ref.label='ar-process-2', echo=FALSE}
```
]

---

## Another Example of an AR(2)<sup>*</sup> Process

.pull-left[
```{r ar-process-3, fig.show='hide'}
set.seed(5)
arima.sim(list(order = c(2, 1, 0),
ar = c(0.6, 0.3)), n = 300) %>%
as_tsibble() %>%
autoplot(value)
```
]

--

.pull-right[
```{r ref.label='ar-process-3', echo=FALSE}
```
]

.footnote[[*]<small>This is technically not an AR(2) model,<br> but rather an ARIMA(2,1,0).<br> We can see the difference in a stationary versus nonstationary series.</small>]

---

## Moving Average (MA) Models

- The "MA" portion of ARIMA refers to a *moving average* model, where $y_{t}$ is regressed on previous forecast errors. 

--

- $y_{t} = c + \varepsilon_{t} + \theta_{1}\varepsilon_{t-1} + ... + \theta_{q}\varepsilon_{t-q}$

--

- This is called an $MA(q)$ model, or a moving average model of order $q$.

--

- Since we don't observe $\varepsilon_{t}$ (the forecast errors are a proxy for $\varepsilon_{t}$), these models aren't a regression in the sense we normally consider.

--

- $\varepsilon_{t}$ is a white noise process with mean 0 and variance/standard deviation of 1.

---

## An Example of an MA(1) Process

.pull-left[
```{r ma-process, fig.show='hide'}
set.seed(5)
arima.sim(list(order = c(0, 0, 1),
ma = 0.5), n = 300) %>%
as_tsibble() %>%
autoplot(value)
```
]

--

.pull-right[
```{r ref.label='ma-process', echo=FALSE}
```
]

---

## An Example of an MA(2) Process

.pull-left[

```{r ma-process-1, fig.show='hide'}
set.seed(5)
arima.sim(list(order = c(0, 0, 2),
ma = c(0.6, 0.3)), n = 300) %>%
as_tsibble() %>%
autoplot(value)
```

]

--

.pull-right[

```{r ref.label='ma-process-1', echo=FALSE}
```

]

---

## Invertibility

- We can express any *stationary* $AR(p)$ model as an $MA(\infty)$ model, using backwards substitution. Below is an example for an $AR(1)$ model.

--

$$\begin{align*}
y_{t} &= \phi_{1}y_{t-1} + \varepsilon_{t}\\ 
    &= \phi_{1}(\phi_{1}y_{t-2} + \varepsilon_{t-1}) + \varepsilon_{t}\\ 
    &= \phi_{1}^2y_{t-2} + \phi_{1}\varepsilon_{t-1} + \varepsilon_{t}\\ 
    &= \phi_{1}^2(\phi_{1}y_{t-3} + \varepsilon_{t-2}) + \phi_{1}\varepsilon_{t-1} + \varepsilon_{t}\\ 
    &= \phi_{1}^3y_{t-3} + \phi_{1}^2\varepsilon_{t-2} + \phi_{1}\varepsilon_{t-1} + \varepsilon_{t}
\end{align*}$$

--

- When $|\phi_{1}| < 1$, $\phi_{1}^k$ will approach zero as $k \to \infty$.
  - $y_{t} = \varepsilon_{t} + \phi_{1}\varepsilon_{t-1} + \phi_{1}^2\varepsilon_{t-2} + ...$
  
---

## Invertibility, cont.

- An $MA(q)$ process can be inverted to an $AR(\infty)$ process if we impose constraints on the parameters. These constraints are similar to those we saw for $AR(p)$ processes.

--

  - The current forecast error is a function of infinite past and current observations of the time series, as seen for an $MA(1)$ model.
  - $\varepsilon_{t} = \sum_{i=0}^\infty(-\theta_{1})^iy_{t-i}$

--

  - When $|\theta_1| < 1$, the impact of observations on the current error declines as we go back in time.
  - If $|\theta_1| > 1$ or $|\theta_1| = 1$, the distant past observations have more or equal weight to the recent observations, and neither situation makes sense.
  
--

  - So an $MA(1)$ process is invertible when $|\theta_1| < 1$.

--

  - Similar to an $AR(2)$ process, an $MA(2)$ process is invertible when $|\theta_2| < 1$, $\theta_1 + \theta_2 > -1$, $\theta_1 - \theta_2 < 1$.
  - Other more complex constraints hold for higher orders of $q$, and the `fable` package handles these.

---

## Putting It All Together

- So far we've discussed $AR$ models, $MA$ models, and differencing time series to ensure stationarity (*I* = *Integration*).

--

- We refer to an $ARIMA(p, d, q)$ model, where $p$ = order of the $AR$ part, $d$ = degree of differencing, and $q$ = order of $MA$ part.

--

$$y'_t = c + \phi_1y'_{t-1} + ... + \phi_py'_{t-p} + \theta_1\varepsilon_{t-1} + ... + \theta_q\varepsilon_{t-q} + \varepsilon_t$$

--

$$(1 - \phi_1B - ... - \phi_pB^p)(1-B)^dy_t = c + (1 + \theta_1B + ... + \theta_qB^q)\varepsilon_t$$

- The same invertibility conditions discussed earlier for AR and MA models apply to the respective parts of ARIMA models.

--

- Special cases of ARIMA models previously discussed:
  - White noise = $ARIMA(0,0,0)$ w/o constant
  - Random walk = $ARIMA(0,1,0)$ w/o constant
  - Random walk w/drift = $ARIMA(0,1,0)$ w/constant
  - Autoregressive model = $ARIMA(p,0,0)$ 
  - Moving average model = $ARIMA(0,0,q)$

---

## Implementation of an ARIMA Model in R

.pull-left[
```{r arima}
train_data <- Sales_tsib %>% 
filter_index(~ "1966 June") # withhold data for testing
fit <- train_data %>% model(ARIMA(Sales))
```
]

--

.pull-right[
```{r arima-show}
report(fit)
```
]

- The `fable` implementation of the Hyndman-Khandakar algorithm selects a model specification based on unit root tests, maximum likelihood estimation, and minimization of the AICc (corrected Akaike Information Criterion which can be a good approximation for cross-validation).
- A stepwise search process is used (unless `stepwise=FALSE` is denoted) so a limited amount of ARIMA models are considered.
- Check out [this link](https://otexts.com/fpp3/arima-r.html) for more info and for details on other arguments.

---

## Residual Review

- It's a good idea to check out the residuals to make sure there is no info contained in the residuals that could be utilized in the model.

--

.pull-left[
```{r resid-review, fig.show='hide'}
ljung.box <- fit %>% augment() %>%
features(.innov, ljung_box, lag = 12, dof = 2)
gg_tsresiduals(fit)
```
```{r ljung-box, echo=FALSE}
knitr::kable(ljung.box, format = "html", align = "c")
```
]

--

.pull-right[
```{r ref.label='resid-review', echo=FALSE, fig.height= 4}
```

]


- Overall, the residuals look good. There are no values outside of the 95% confidence interval around zero in the ACF plot, although the residuals are not quite normally distributed.

---

## Forecasting with ARIMA Models

- Forecasting is extremely simple with an ARIMA model (and really all models in the `fable` packages).
- You can use English language for the `h` argument, or a number of periods you want to forecast. 
- 80% and 95% intervals are included automatically. Specify `level = NULL` to not show these on the plot.
- Note that to plot both the historical series and the forecast, you need to specify the data as the first argument of `autoplot()`.

--

.pull-left[
```{r forecast-arima, fig.show='hide'}
fit %>% 
forecast(h = "1 year") %>%
autoplot(Sales_tsib, level=95)
```
]

--

.pull-right[
```{r ref.label='forecast-arima', echo=FALSE, fig.height=5}
```
]

---

## Checking the Accuracy of Our Model

- When creating a model, measuring prediction/forecast accuracy is key. 
- We withheld data to use for testing our forecast. This is called a *validation set approach*.
- There are other cross-validation approaches, but these are outside of the scope of this presentation.

```{r accuracy-arima}
fcst <- fit %>% forecast(h = "1 year")
fcst %>% accuracy(Sales_tsib) %>% knitr::kable(format = "html", align = "c")
```

- RMSE and MASE are commonly utilized accuracy measures. RMSE (Root Mean Square Error) is in the units of the variable of interest. 
- MASE is a metric that measures how well a forecast performs relative to a naive forecast. A value $<1$ means that the forecast performs better than a naive forecast.

---

## But What If I Want to Specify The Model Myself?

- "Back in my day", we didn't have `fable` to select an ARIMA model for us, so we had to figure it out ourselves.

--

- The traditional way is to rely on the ACF and PACF plots as a starting point.
  - PACF stands for "Partial Autocorrelation Function", and is an autocorrelation function which controls for the effect of $y_{t-k}$ on $y_t$, removing the effects of lags $1...k-1$ on $y_t$.
  - The partial autocorrelation for lag $k = \phi_k$, so it represents the $k$th (i.e., last) $p$ in an $AR(k)$ model.
  - Thus, we look at the last significant partial autocorrelation to give us an idea of the order of $p$.

--

- If the data follows an $ARIMA(p,d,0)$ model, then for the **differenced** data:
  - The ACF decays in an exponential or sinusoidal manner.
  - The last significant spike in the PACF is at lag $p$, with nothing after.
  
--

- If the data follows an $ARIMA(0,d,q)$ model, then for the **differenced** data:
  - The last significant spike on the ACF is at lag $q$, with nothing after.
  - The PACF decays in an exponential or sinusoidal way.

--

- However, these are not always informative, especially in cases where both $p$ and $q$ are not zero.

---

## Reviewing the PACF and ACF in R

- Consider the sales data we previously modeled.
- We know it is nonstationary and needs to be differenced once.

.pull-left[
```{r pacf-acf, fig.show='hide'}
train_data %>% 
gg_tsdisplay(difference(Sales), 
plot_type = "partial")
```

- Neither the PACF nor the ACF seem to decay exponentially.

- We could naively go with an $ARIMA(2,1,4)$ model, but this may be too complex.
]

.pull-right[
```{r ref.label='pacf-acf', echo=FALSE, fig.height=6}
```
]
