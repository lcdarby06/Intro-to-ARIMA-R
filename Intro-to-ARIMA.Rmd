---
title: "Introduction to ARIMA Models Using R"
author: "Laura Rose"
date: "April 19, 2022"
output: 
  xaringan::moon_reader:
    css: [default, rladies-fonts, rladies]
    lib_dir: libs
    nature:
      countIncrementalSlides: false
      ratio: '16:9'
      
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, fig.retina = 3, warning = FALSE, message = FALSE)
package_list <- c("fable", "feasts", "tsibble", "tidyverse")
for (i in package_list){
  if (!require(i, character.only = TRUE)){
    install.packages(i, repos = 'http://cran.us.r-project.org', dependencies = TRUE)
  library(i, character.only = TRUE)
  }
}

```

## Intro to ARIMA Models
- ARIMA models are one of the most common univariate time series forecasting methods.

--

- ARIMA stands for **A**uto**R**egressive **I**ntegrated **M**oving **A**verage.

--

- We will explore each part of the ARIMA model in detail before moving on to its implementation in R.

---

## For Further Reading
- Hyndman, R.J., & Athanasopoulos, G. (2021) *Forecasting: principles and practice*, 3rd edition, OTexts: Melbourne, Australia. OTexts.com/fpp3. Accessed on March 3, 2022.
- Brooks, C. (2008). *Introductory Econometrics for Finance*, 2nd edition, Cambridge University Press: Cambridge, United Kingdom.

---

## Stationarity
- A time series is said to be *strictly stationary* if the distribution of the observations does not change across time.

--

- However, strict stationarity is not always necessary to model a time series. We often reference *weak stationarity* in our analysis.

--

- $E(y_{t}) = \mu$ (constant mean)

--

- $E[(y_{t} - \mu)(y_{t} - \mu)] = \sigma^2 < \infty$ (constant variance)

--

- $E[(y_{t} - \mu)(y_{t'} - \mu)] = \gamma_{t-t'} \space \forall \space t, t'$ (constant autocovariance)

--
  - When $t = t'$, the autocovariance is the variance.
  
--

- The autocovariance is not particularly useful since it is scale-dependent. Therefore, we often normalize the autocovariance by dividing by the variance.

--

- This is called the *autocorrelation*, and this measure has the usefulness of being bounded between $\pm1$

---

## White Noise Process

- A *white noise process* is a special case of a stationary process.

--

$$E(y_{t}) = 0$$

--

$$E[(y_{t} - \mu)(y_{t} - \mu)] = \sigma^2$$

--

$$\gamma_{k-k'} = \left\{\begin{aligned}
&\sigma^2 &&if \space k = k'\\
&0 && otherwise
\end{aligned}
\right.$$

--

- *Note that some definitions of a white noise process indicate a nonzero mean is permissible.*

---

## Random Walk 

- A *random walk* model is often used to denote a nonstationary time series.

--

- $y_{t} = y_{t-1} + \varepsilon_{t}$ or

--

- $y_{t} = c + y_{t-1} + \varepsilon_{t}$ (random walk w/drift)
  - $c$ is the mean of the changes between sequential observations.
  - If $c > 0$, the mean change will tend towards an increase in $y_{t}$, and vice versa for negative values of $c$. 
  
--

- Random walk series often have long period of increases or decreases accompanied with sudden changes in direction.

---

## What to Do If Data is Nonstationary?
- So what happens if we find our data violates any of the (weak) stationarity conditions?

--

- Note that data with trend or seasonality is not stationary, but data with cyclic behavior (i.e., business or other cycles which are not of fixed length) can be stationary.

--

- Differencing the series is a common way to deal with nonstationarity resulting from autocorrelation. 

--

  - The difference of a value with the previous value is called *first-order differencing*: $y'_{t} = y_{t} - y_{t-1}$.
  - Note that for a random walk model, $y'_{t} = \varepsilon_{t}$. 
  - Thus, this series is now stationary given the properties of $\varepsilon_{t}$.
  
--

  - Sometimes this is not enough to make the series stationary, so we take the difference of the differences.

--

$$\begin{align*}
  y''_{t}  &=  y'_{t}  - y'_{t - 1} \\
           &= (y_t - y_{t-1}) - (y_{t-1}-y_{t-2})\\
           &= y_t - 2y_{t-1} +y_{t-2}
\end{align*}$$

---

## What to Do if Data is Nonstationary, cont.

- If we notice a seasonal pattern, we can take a seasonal difference: $y'_{t} = y_{t} - y_{t-m}$, where $m$ is the seasonal period. 

--

- If the data looks like it may require both first-differencing and seasonal differencing, it's better to take the seasonal difference first.

--

- This is especially true in the case of strong seasonality, since sometimes taking a seasonal difference is enough to make the series stationary.

--

- However, taking a first difference will not get rid of seasonal stationarity.

--

- Always use as few differences as possible, since too much differencing can induce patterns in the series that are not actually there.

--

- If the variance appears nonconstant across time, a logarithmic transformation can be used to stabilize.

---

## How to Test for Nonstationarity

- There are both formal and nonformal ways to examine the time series.

--

- We should always start by plotting the time series (line plot is generally best), since often nonstationarity and nonconstant variance are visible to the human eye.

--

- If we suspect nonstationarity, we should plot the autocorrelation function (ACF) and check for observations outside of the 95% confidence interval around 0. We particularly want to examine how quickly these autocorrelations drop to 0.

--

- To formally test for stationarity, we conduct a *unit root test*.

--

- Taking a cue from Dr. Hyndman, we will use the KPSS test, but other tests are available (ADF and PP tests).

--

- A *unit root* implies that the data is essentially a function of its lag(s) where the coefficient equals 1, plus some noise. (There are some variations on this, but we will skip discussion for the sake of simplicity.)
  - In more formal terms, the root of the characteristic equation equals 1.

---

## An Example of Checking for Stationarity

.pull-left[
```{r plot-last, fig.show='hide', warning=FALSE, message=FALSE}
data(BJsales)
date_sequence <- seq(as.Date("1955-01-01"), 
by = "month", length.out = 150)
Sales_tsib <- tsibble(Month = yearmonth(date_sequence), 
Sales = BJsales, index = Month)
autoplot(Sales_tsib)
```
]

.pull-right[
```{r ref.label='plot-last', echo=FALSE}
```
]

---

## Checking the ACF

.pull-left[
```{r plot-last2, fig.show='hide'}
Sales_tsib %>% ACF() %>% autoplot()
```
]

--

.pull-right[
```{r ref.label='plot-last2', echo=FALSE}
```
]

---

## Testing for a Unit Root 

```{r uroot}
uroot_table <- Sales_tsib %>% 
features(Sales, unitroot_kpss) %>% 
bind_cols(Sales_tsib %>%
features(Sales, unitroot_nsdiffs)) %>% 
bind_cols(Sales_tsib %>% 
features(difference(Sales, 12), unitroot_ndiffs))
knitr::kable(uroot_table, format = "html",
digits = 2, align = "c")
```

--

- We clearly have a unit root (we reject the null hypothesis of stationarity), but it doesn't look like we will need to take a seasonal difference.

---

## Backshift Notation

- This notation is a (hopefully!) helpful shorthand to describe lags of variables for easier manipulation. Think of $B$ as "backshift."

--

- $By_{t} = y_{t-1}$

--

- By extension, $B(By_{t}) = B^2y_{t} = y_{t-2}$, etc.

--

- Same quarter last year would be $B^4y_{t} = y_{t-4}$

--

- We can also use the backshift operator to describe differencing.
$$y' = y_{t} - y_{t-1} = y_{t} - By_{t} = (1-B)y_{t}$$
- We can treat the expressions in $B$ like polynomials and solve for roots (thus the unit root concept).
  
--

$$\begin{align*}
y'' &= (y_{t} - y_{t-1}) - (y_{t-1} - y_{t-2})\\ 
    &= y_{t} - By_{t} - By_{t} + B^2y_{t}\\ 
    &= y_{t} - 2By_{t} + B^2y_{t}\\ 
    &= (B-1)^2y_{t}\\ 
    &= (1-B)^2y_{t}
\end{align*}$$
  
---

## Autoregressive (AR) Models

- With an AR model, we forecast a time series using a linear combination of prior values of the series. In other words, $y_{t}$ is regressed on itself.

--

- An autoregressive model of order $p$ (an $AR(p)$) model is written as follows, where $\varepsilon_{t}$ is a white noise process:

--

$$y_{t} = c + \phi_{1}y_{t-1} + \phi_{2}y_{t-2} + ... + \phi_{p}y_{t-p} + \varepsilon_{t}$$
--

- $AR(1)$ models as they relate to previous models we've discussed:
  - when $\phi_{1} = 0$ and $c = 0$, $y_{t}$ is white noise
  - when $\phi_{1} = 1$ and $c = 0$, $y_{t}$ is a random walk process
  - when $\phi_{1} = 1$ and $c \neq 0$, $y_{t}$ is a random walk with drift process
  - when $\phi_{1} < 0$, $y_{t}$ tends to fluctuate around the mean

--

- $AR(p)$ models, stationary data, and constraints:
  - $AR(1)$ model: $-1 < \phi_{1} < 1$
  - $AR(2)$ model: $-1 < \phi_{2} < 1$, $\phi_{1} + \phi_{2} < 1$, $\phi_{1} - \phi_{2} < 1$

--

- These restrictions, as well as restrictions for higher orders of $p$, are handled automatically in the `fable` package.

---

## An Example of AR(1) Process

.pull-left[
```{r ar-process, fig.show='hide'}
set.seed(5)
arima.sim(list(order = c(1, 0, 0),
ar = 0.7), n = 300) %>% as_tsibble() %>%
autoplot(value)
```
]

--

.pull-right[
```{r ref.label='ar-process', echo=FALSE}
```
]

---

## Another Example of AR(1)<sup>*</sup> Process

.pull-left[
```{r ar-process-1, fig.show='hide'}
set.seed(5)
arima.sim(list(order = c(1, 1, 0),
ar = 0.7), n = 300) %>%
as_tsibble() %>%
autoplot(value)
```
]

--

.pull-right[
```{r ref.label='ar-process-1', echo=FALSE}
```
]

.footnote[[*]<small>This is technically not an AR(1) model,<br> but rather an ARIMA(1,1,0).<br> We can see the difference in a stationary versus nonstationary series.</small>]

---

## An Example of an AR(2) Process

.pull-left[
```{r ar-process-2, fig.show='hide'}
set.seed(5)
arima.sim(list(order = c(2, 0, 0),
ar = c(1.3, -0.6)), n = 300) %>%
as_tsibble() %>%
autoplot(value)
```
]

--

.pull-right[
```{r ref.label='ar-process-2', echo=FALSE}
```
]

---

## Another Example of an AR(2)<sup>*</sup> Process

.pull-left[
```{r ar-process-3, fig.show='hide'}
set.seed(5)
arima.sim(list(order = c(2, 1, 0),
ar = c(0.6, 0.3)), n = 300) %>%
as_tsibble() %>%
autoplot(value)
```
]

--

.pull-right[
```{r ref.label='ar-process-3', echo=FALSE}
```
]

.footnote[[*]<small>This is technically not an AR(2) model,<br> but rather an ARIMA(2,1,0).<br> We can see the difference in a stationary versus nonstationary series.</small>]

---

## Moving Average (MA) Models

- The "MA" portion of ARIMA refers to a *moving average* model, where $y_{t}$ is regressed on previous forecast errors. 

--

- $y_{t} = c + \varepsilon_{t} + \theta_{1}\varepsilon_{t-1} + ... + \theta_{q}\varepsilon_{t-q}$

--

- This is called an $MA(q)$ model, or a moving average model of order $q$.

--

- Since we don't observe $\varepsilon_{t}$ (the forecast errors are a proxy for $\varepsilon_{t}$), these models aren't a regression in the sense we normally consider.

--

- $\varepsilon_{t}$ is a white noise process with mean 0 and variance/standard deviation of 1.

---

## An Example of an MA(1) Process

.pull-left[
```{r ma-process, fig.show='hide'}
set.seed(5)
arima.sim(list(order = c(0, 0, 1),
ma = 0.5), n = 300) %>%
as_tsibble() %>%
autoplot(value)
```
]

--

.pull-right[
```{r ref.label='ma-process', echo=FALSE}
```
]

---

## An Example of an MA(2) Process

.pull-left[

```{r ma-process-1, fig.show='hide'}
set.seed(5)
arima.sim(list(order = c(0, 0, 2),
ma = c(0.6, 0.3)), n = 300) %>%
as_tsibble() %>%
autoplot(value)
```

]

--

.pull-right[

```{r ref.label='ma-process-1', echo=FALSE}
```

]

---

## Invertibility

- We can express any *stationary* $AR(p)$ model as an $MA(\infty)$ model, using backwards substitution. Below is an example for an $AR(1)$ model.

--

$$\begin{align*}
y_{t} &= \phi_{1}y_{t-1} + \varepsilon_{t}\\ 
    &= \phi_{1}(\phi_{1}y_{t-2} + \varepsilon_{t-1}) + \varepsilon_{t}\\ 
    &= \phi_{1}^2y_{t-2} + \phi_{1}\varepsilon_{t-1} + \varepsilon_{t}\\ 
    &= \phi_{1}^2(\phi_{1}y_{t-3} + \varepsilon_{t-2}) + \phi_{1}\varepsilon_{t-1} + \varepsilon_{t}\\ 
    &= \phi_{1}^3y_{t-3} + \phi_{1}^2\varepsilon_{t-2} + \phi_{1}\varepsilon_{t-1} + \varepsilon_{t}
\end{align*}$$

--

- When $|\phi_{1}| < 1$, $\phi_{1}^k$ will approach zero as $k \to \infty$.
  - $y_{t} = \varepsilon_{t} + \phi_{1}\varepsilon_{t-1} + \phi_{1}^2\varepsilon_{t-2} + ...$
  
---

## Invertibility, cont.

- An $MA(q)$ process can be inverted to an $AR(\infty)$ process if we impose constraints on the parameters. These constraints are similar to those we saw for $AR(p)$ processes.

--

  - The current forecast error is a function of infinite past and current observations of the time series, as seen for an $MA(1)$ model.
  - $\varepsilon_{t} = \sum_{i=0}^\infty(-\theta_{1})^iy_{t-i}$

--

  - When $|\theta_1| < 1$, the impact of observations on the current error declines as we go back in time.
  - If $|\theta_1| > 1$ or $|\theta_1| = 1$, the distant past observations have more or equal weight to the recent observations, and neither situation makes sense.
  
--

  - So an $MA(1)$ process is invertible when $|\theta_1| < 1$.

--

  - Similar to an $AR(2)$ process, an $MA(2)$ process is invertible when $|\theta_2| < 1$, $\theta_1 + \theta_2 > -1$, $\theta_1 - \theta_2 < 1$.
  - Other more complex constraints hold for higher orders of $q$, and the `fable` package handles these.

---

## Putting It All Together

- So far we've discussed $AR$ models, $MA$ models, and differencing time series to ensure stationarity (*I* = *Integration*).

--

- We refer to an $ARIMA(p, d, q)$ model, where $p$ = order of the $AR$ part, $d$ = degree of differencing, and $q$ = order of $MA$ part.

--

$$y'_t = c + \phi_1y'_{t-1} + ... + \phi_py'_{t-p} + \theta_1\varepsilon_{t-1} + ... + \theta_q\varepsilon_{t-q} + \varepsilon_t$$

--

$$(1 - \phi_1B - ... - \phi_pB^p)(1-B)^dy_t = c + (1 + \theta_1B + ... + \theta_qB^q)\varepsilon_t$$

- The same invertibility conditions discussed earlier for AR and MA models apply to the respective parts of ARIMA models.

--

- Special cases of ARIMA models previously discussed:
  - White noise = $ARIMA(0,0,0)$ w/o constant
  - Random walk = $ARIMA(0,1,0)$ w/o constant
  - Random walk w/drift = $ARIMA(0,1,0)$ w/constant
  - Autoregressive model = $ARIMA(p,0,0)$ 
  - Moving average model = $ARIMA(0,0,q)$

---

## Implementation of an ARIMA Model in R

.pull-left[
```{r arima}
train_data <- Sales_tsib %>% 
filter_index(~ "1966 June") # withhold data for testing
fit <- train_data %>% model(ARIMA(Sales))
```
]

--

.pull-right[
```{r arima-show}
report(fit)
```
]

- The `fable` implementation of the Hyndman-Khandakar algorithm selects a model specification based on unit root tests, maximum likelihood estimation, and minimization of the AICc (corrected Akaike Information Criterion which can be a good approximation for cross-validation).
- A stepwise search process is used (unless `stepwise=FALSE` is denoted) so a limited amount of ARIMA models are considered.
- Check out [this link](https://otexts.com/fpp3/arima-r.html) for more info and for details on other arguments.

---

## Residual Review

- It's a good idea to check out the residuals to make sure there is no info contained in the residuals that could be utilized in the model.

--

.pull-left[
```{r resid-review, fig.show='hide'}
ljung.box <- fit %>% augment() %>%
features(.innov, ljung_box, lag = 12, dof = 2)
gg_tsresiduals(fit)
```
```{r ljung-box, echo=FALSE}
knitr::kable(ljung.box, format = "html", align = "c")
```
]

--

.pull-right[
```{r ref.label='resid-review', echo=FALSE, fig.height= 4}
```

]


- Overall, the residuals look good. There are no values outside of the 95% confidence interval around zero in the ACF plot, although the residuals are not quite normally distributed.

---

## Forecasting with ARIMA Models

- Forecasting is extremely simple with an ARIMA model (and really all models in the `fable` packages).
- You can use English language for the `h` argument, or a number of periods you want to forecast. 
- 80% and 95% intervals are included automatically. Specify `level = NULL` to not show these on the plot.
- Note that to plot both the historical series and the forecast, you need to specify the data as the first argument of `autoplot()`.

--

.pull-left[
```{r forecast-arima, fig.show='hide'}
fit %>% 
forecast(h = "1 year") %>%
autoplot(Sales_tsib, level=95)
```
]

--

.pull-right[
```{r ref.label='forecast-arima', echo=FALSE, fig.height=5}
```
]

---

## Checking the Accuracy of Our Model

- When creating a model, measuring prediction/forecast accuracy is key. 
- We withheld data to use for testing our forecast. This is called a *validation set approach*.
- There are other cross-validation approaches, but these are outside of the scope of this presentation.

```{r accuracy-arima}
fcst <- fit %>% forecast(h = "1 year")
fcst %>% accuracy(Sales_tsib) %>% knitr::kable(format = "html", align = "c", digits = 2)
```

- RMSE and MASE are commonly utilized accuracy measures. RMSE (Root Mean Square Error) is in the units of the variable of interest. 
- MASE is a metric that measures how well a forecast performs relative to a naive forecast. A value $<1$ means that the forecast performs better than a naive forecast.

---

## But What If I Want to Specify The Model Myself?

- "Back in my day", we didn't have `fable` to select an ARIMA model for us, so we had to figure it out ourselves.

--

- The traditional way is to rely on the ACF and PACF plots as a starting point.
  - PACF stands for "Partial Autocorrelation Function", and is an autocorrelation function which controls for the effect of $y_{t-k}$ on $y_t$, removing the effects of lags $1...k-1$ on $y_t$.
  - The partial autocorrelation for lag $k = \phi_k$, so it represents the $k$th (i.e., last) $p$ in an $AR(k)$ model.
  - Thus, we look at the last significant partial autocorrelation to give us an idea of the order of $p$.

--

- If the data follows an $ARIMA(p,d,0)$ model, then for the **differenced** data:
  - The ACF decays in an exponential or sinusoidal manner.
  - The last significant spike in the PACF is at lag $p$, with nothing after.
  
--

- If the data follows an $ARIMA(0,d,q)$ model, then for the **differenced** data:
  - The last significant spike on the ACF is at lag $q$, with nothing after.
  - The PACF decays in an exponential or sinusoidal way.

--

- However, these are not always informative, especially in cases where both $p$ and $q$ are not zero.

---

## Reviewing the PACF and ACF in R

- Consider the sales data we previously modeled.
- We know it is nonstationary and needs to be differenced once.

.pull-left[
```{r pacf-acf, fig.show='hide'}
train_data %>% 
gg_tsdisplay(difference(Sales), 
plot_type = "partial")
```

- Neither the PACF nor the ACF seem to decay exponentially.

- We could naively go with an $ARIMA(2,1,4)$ model, but this may be too complex.

- It is better to try out both an $ARIMA(2,1,0)$ and $ARIMA(0,1,4)$ first. 
]

.pull-right[
```{r ref.label='pacf-acf', echo=FALSE, fig.height=6}
```
]

---

## Model Estimation and Forecasting

.pull-left[
```{r model-estim}
models <- train_data %>% 
model(arima210 = ARIMA(Sales ~ pdq(2,1,0)),
arima014 = ARIMA(Sales ~ pdq(0,1,4)),
arima_auto = ARIMA(Sales))  
models %>% glance() %>% 
arrange(AICc) %>% 
select(.model:BIC) %>% 
knitr::kable(format = "html", align = "c", digits = 2)
```
]

--

.pull-right[
```{r resid-display, fig.height=5}
models %>% select(arima_auto) %>% 
gg_tsresiduals()
```
]

---
  
## Summary of Procedures for Modeling an ARIMA Process

.pull-left[
### Automatic Process
- Transform data if necessary to obtain constant variance.
- The `ARIMA()` procedure handles differencing for stationarity and order selection.
- Check residuals from model to ensure they are white noise.
- Produce forecasts from automatically selected model.
]

--

.pull-right[
### Hands-On Process
- Transform data if necessary to obtain constant variance.
- Review data for stationarity (visual inspection, KPSS tests, etc.).
- Review ACF and PACF plots and make some initial model guesses. 
- Compare several reasonable candidate models (including the automatically selected model), and choose the once which minimizes the AICc (or another information criterion). 
- Check the residuals from this model to ensure they are white noise.
- Produce forecasts from the selected model. 
]

---

## The Role of the Constant and Difference in the Model

- We can control whether or not a constant is included in the model by the use of 0 preceding the `pdq(p,d,q)` special. 
- `model(my_arima = ARIMA(my_var ~ 0 + pdq(p,d,q)))` to exclude a constant or `model(my_arima = ARIMA(my_var ~ 1 + pdq(p,d,q)))` to include a constant
- If neither 0 nor 1 is specified, the constant is included depending on if its inclusion minimizes the AICc or not, for $d=0$ or $d=1$. For $d>1$, no constant is included. 
- The constant can also affect the long-term forecasts.
- $c=0$:
  - $d=0$ -> forecasts go to zero.
  - $d=1$ -> forecasts go to a nonzero constant.
  - $d=2$ -> forecasts become a straight line.
- $c\neq0$:
  - $d=0$ -> forecasts go to mean of data.
  - $d=1$ -> forecasts follow straight line.
  - $d=2$ -> this is not allowed in the `fable::ARIMA()` function, but the forecasts would follow a quadratic trend.
- The degree of $d$ also matters to the long-term forecasts. The greater the value of $d$, the more quickly prediction intervals increase in size.

---

## How Degree of $d$ Affects Prediction Intervals

.pull-left[
```{r dod, fig.show='hide'}
train_data %>% 
gg_tsdisplay(difference(Sales, differences = 2), plot_type = "partial")
```
]

--

.pull-right[
```{r ref.label='dod', echo=FALSE, fig.height=6}
```
- An `ARIMA(0,2,1)` looks reasonable.
]

---

## How Degree of $d$ Affects Prediction Intervals, cont.

.pull-left[
```{r arima-d, fig.show='hide'}
my_models <- train_data %>% 
model(arima111 = ARIMA(Sales ~ pdq(1,1,1)),
arima021 = ARIMA(Sales ~ pdq(0,2,1)))
my_models %>% 
forecast(h = "1 year") %>% 
autoplot(Sales_tsib, level = 95)
```
]

--

.pull-right[
```{r ref.label='arima-d', echo=FALSE}
```
]

---

## What About Seasonal Models?

- We briefly touched on seasonal ARIMA models earlier in the session.
- It is usually obvious if there is (strong) seasonality in the data, due to spikes at lags on the PACF/ACF plots, as well as patterns in a simple line plot of the data.
- The sales data we previously considered does not have a seasonal pattern, so let's take a look at some seasonal data.
- We will use quarterly tourism data from Australia for the purpose of Holiday.

---

## Modeling Seasonal Data

.pull-left[
```{r tourism, fig.show='hide'}
h_tourism <- tourism %>% 
group_by(Purpose) %>% 
summarize(Trips = sum(Trips)) %>% 
filter(Purpose == "Holiday") 
h_tourism_train <- h_tourism %>% 
filter_index(. ~ "2016 Q4")
h_tourism %>% 
autoplot(Trips)
```
]

--

.pull-right[
```{r ref.label='tourism', echo=FALSE, fig.height=6}
```
- Note the slight nonconstant variance; we will need to transform the series prior to modeling.
]

---

## Transforming the Data to Stabilize Variance

.pull-left[
```{r lambda, fig.show='hide'}
lambda_value <- h_tourism_train %>% 
features(Trips, guerrero) %>% 
pull(lambda_guerrero)
h_tourism %>% 
autoplot(box_cox(Trips, lambda_value))
```
]

--

.pull-right[
```{r ref.label='lambda', echo=FALSE, fig.height=5}
```
- This doesn't seem to have done a lot to stabilize the variance, but it may be the best we can do.
- Data is still nonstationary, so we will need to difference.
]

---

## Differencing Seasonal Data

- We will try seasonal differencing first and then see if we also need to first-difference the data.

--

```{r seas-diff}
## table for reviewing unit root results
bind_cols(h_tourism_train %>% 
features(Trips, unitroot_nsdiffs),
h_tourism_train %>% 
features(difference(Trips, 4), unitroot_ndiffs) %>%
select(ndiffs)) %>%
knitr::kable(format = "html",
align = "c")
```

---

## Differencing Seasonal Data, cont.

.pull-left[
```{r seas-diff2, fig.show='hide'}
## adding transformed vars to data set
h_tourism_train <- h_tourism_train %>% 
mutate(Trips_t = box_cox(Trips, lambda_value)) %>% 
mutate(Trips_t_s = difference(Trips_t, 4),
Trips_t_s_d = difference(Trips_t_s, 1))

## visual inspection for stationarity
h_tourism_train %>% 
pivot_longer(-c(Purpose, Quarter),
names_to = 'Variable', values_to = 'Trips') %>% 
mutate(Variable = as.factor(Variable)) %>% 
ggplot(aes(Quarter, Trips)) +
geom_line() +
facet_grid(vars(Variable), scales = 'free_y')

h_tourism_train <- h_tourism_train %>% select(-c(Trips_t:Trips_t_s_d)) # removing vars we don't need
```
]

--

.pull-right[
```{r ref.label='seas-diff2', echo=FALSE, fig.height=6}
```
]

--

- Visually, there seems to be some stationarity benefit from taking a first difference in addition to the seasonal difference.

---

## Examination of the PACF/ACF Plots

.pull-left[
```{r seas-pacf-acf, fig.show='hide'}
## note that we are using the transformed, 
## seasonally differenced, and first-differenced variable 
h_tourism_train %>% 
gg_tsdisplay(difference(Trips, 4) %>% difference(), plot_type = "partial", lag_max = 20)
```
]

--

.pull-right[
```{r ref.label='seas-pacf-acf', echo=FALSE, fig.height=6}
```
]

--

- It looks like there are lag spikes on the ACF plot on lags 1 and 4 and on lags 1, 2, and 4 on the PACF plot. This suggests we try an $ARIMA(2,1,1)(1,1,1)_{4}$ model. 

---

## Seasonal ARIMA Modelling

.pull-left[
```{r seas-arima-mod}
seas_mods <- h_tourism_train %>% 
model(arima211111 = 
ARIMA(box_cox(Trips, lambda_value) ~ pdq(2,1,1) + PDQ(1,1,1)), # use the transformed, but not differenced variable since differences specified in formula
auto_arima = ARIMA(box_cox(Trips, lambda_value)))

seas_mods %>% glance() %>% 
arrange(AICc) %>% 
select(.model:BIC) %>% 
knitr::kable(format = 'html', align = 'c',
digits = 2)
```
]

--

.pull-right[
```{r seas-arima-mod-2}
seas_mods %>% pivot_longer(-Purpose) %>% 
knitr::kable(format = 'html', align='c', digits = 2)
```

--

- The automatically selected, simpler model has a lower AICc than the model we specified, so we will choose that model.

]

---

## Residual Review for Seasonal Model

.pull-left[
```{r seas-resid, fig.show='hide'}
seas_mods %>% select(auto_arima) %>% 
gg_tsresiduals()
```
]

--

.pull-right[
```{r ref.label='seas-resid', echo=FALSE}
```
]

--

- The residuals look very good, so we are okay to proceed with forecasting.

---

## Forecasting with a Seasonal Model

.pull-left[
```{r seas-forecast, fig.show='hide'}
h_tourism_train %>% #not using the seas_mods object
#since it makes the back transformations more difficult
model(ARIMA(box_cox(Trips, lambda_value))) %>% 
forecast(h = "1 year") %>% 
autoplot(h_tourism,level = 95)
```
]

--

.pull-right[
```{r ref.label='seas-forecast', echo=FALSE, fig.height=6}
```
]
- The model definitely under-predicts tourism in the winter and spring months (Q3 and Q4). This makes sense given past trends, but this is a case of where a model with external regressors may be helpful in predicting this uptick in travel.

---

## Forecast Accuracy for a Seasonal Model

- The framework is generally the same as with the nonseasonal model. For comparison, we are estimating a seasonal naive model to see if our model outperforms.

```{r seas-accuracy}
h_tourism_train %>% 
model(ARIMA(box_cox(Trips, lambda_value)),
SNAIVE(Trips)) %>%
forecast(h="1 year") %>% 
accuracy(h_tourism) %>% 
select(.model, RMSE, MASE) %>% 
knitr::kable(format = "html", align = "c", digits = 2)
```

- The accuracy isn't great, but at least it's better than a seasonal naive model.

---

## Estimating Models for Multiple Series

- It's easy in `fable` to estimate models for many time series at once. 
- If we have a `tsibble` with multiple series, and key structure denoted accordingly, `fable` can figure out that we want a separate model(s) for each series.
- Note that for this to work, the data will need to be in long format.

---

## Estimating Models for Multiple Series, cont.

.pull-left[
```{r mult-fable}
tourism_train <- tourism %>% 
filter_index(. ~ "2016 Q4")

accuracy_table <- tourism_train %>% 
filter(Purpose == "Holiday") %>% 
model(ARIMA(Trips),
SNAIVE(Trips)) %>% 
forecast(h = "1 year") %>% 
accuracy(tourism) %>% 
select(.model:Purpose, RMSE, MASE) %>% 
arrange(State, Region,Purpose, .model) %>% 
slice_head(n=10) #there are actually 304 series
```
]

--

.pull-right[
```{r mult-fable-table}
accuracy_table %>% 
knitr::kable(format = "html", align = "c", digits = 2)
```
]

- Sometimes the `ARIMA()` model is more accurate than the `SNAIVE()` model, and sometimes not. This also varies depending on metric considered.

---

## Conclusion

- ARIMA models are one of the most commonly used univariate time series methods.
- These models can be useful when:
  - The data are believed to be a function of previous values and/or previous errors.
  - You don't have access to/are unaware of other predictor variables.
- `fable`,`feasts`, and `tsibble` allow you to produce many ARIMA models/forecasts quickly with minimal code, and no use of `purrr`, `for` loops, etc. required.

---

class: center, middle, inverse

# Appendix

---

## Parameter Estimation with `fable`

- Once we have selected the order of the model, the parameter values need to be estimated.
- Maximum Likelihood Estimation (MLE) is commonly used. 
- MLE searches for values of the parameters $c$, $\phi_1....\phi_p$, and $\theta_1...\theta_q$ which maximize the probability that the data observed arose from a process with such parameters.
- In the ARIMA context, MLE is similar to least squares (minimizing the sum of the squared residuals).
- Note that when we call `report()` on a `mable` (`fable` model object), we see an entry for *log-likelihood*. 
- This refers to the logarithm of the likelihood that the data come from the estimated model. 
- We want to maximize this value, though AICc (next slide) is more commonly used as a measure of model accuracy.

---

## Information Criteria

- We previously utilized the AICc in comparing models.
- The AICc (Akaike Corrected Information Criterion) is one of several information criteria used to select a good model.
- Note that the $y_t$ variable must be the same to compare between models.
  - We cannot use the AICc or other information criterion to compare between models with different $d$.
- The AIC (not corrected) is basically -2*log-likelihood + 2 times the number of parameters including $\sigma$. 
  - The AICc more heavily penalizes for the number of parameters, especially as it relates to the observations in the model. 
- We want to minimize the information criterion (consider the -2 in the equation), so a negative number with a higher magnitude is better.

---

## Prediction Intervals

- As mentioned previously, the width and increase of width of prediction intervals is related to $d$. 
- In general, prediction interval calculation for ARIMA models is complex, particularly beyond the first period.
  - For the first period, the 95% prediction interval is $\hat{y}_{T+1} \pm 1.96\hat{\sigma}$, where $\hat{\sigma}$ is the standard deviation of the residuals. Consider that this is because the inputs we have for the model are known, but beyond $T+1$ we have to use forecasted inputs for certain or all values.
- For an $ARIMA(0,0,q)$ model, the forecast variance at time $T+h$ can be expressed as follows:
$$\hat{\sigma}^2_h = \hat{\sigma}^2 + \hat{\sigma}^2 \sum_{i=1}^{h-1}\theta^2_i$$ for $h>1$
- Recall that a stationary $AR(p)$ model can be inverted as an $MA(\infty)$ model, so the equation above shows us a way to calculate intervals for stationary $AR(p)$ models.

  


  
  
  